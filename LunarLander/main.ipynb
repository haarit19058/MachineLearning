{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/haarit/myenv/lib/python3.13/site-packages (1.0.0)\n",
      "Requirement already satisfied: torch in /home/haarit/myenv/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /home/haarit/myenv/lib/python3.13/site-packages (2.2.1)\n",
      "Collecting swig\n",
      "  Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/haarit/myenv/lib/python3.13/site-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/haarit/myenv/lib/python3.13/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/haarit/myenv/lib/python3.13/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: setuptools in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/haarit/myenv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/haarit/myenv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygame>=2.1.3 (from gymnasium[box2d])\n",
      "  Using cached pygame-2.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/haarit/myenv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
      "Using cached pygame-2.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp313-cp313-linux_x86_64.whl size=2571560 sha256=83d3b8d62e38ca63a4a31349afe8975e9efd88613da072d097339ce2ced013a7\n",
      "  Stored in directory: /home/haarit/.cache/pip/wheels/ac/ff/f1/2790d9b5c2f79b5fbece7d941e4d3f94ffab9993a7bcc610b3\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: swig, box2d-py, pygame\n",
      "Successfully installed box2d-py-2.3.5 pygame-2.6.1 swig-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium torch numpy swig gymnasium[box2d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dimensions: 8\n",
      "Action Dimensions: 4\n",
      "Final Score: -90.30743934077333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haarit/myenv/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:672: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Lunar Lander environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "# Check environment details\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State Dimensions: {state_dim}\")\n",
    "print(f\"Action Dimensions: {action_dim}\")\n",
    "\n",
    "# Test the environment with random actions\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()  # Renders the environment\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    score += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"Final Score: {score}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: -92.66950692436035, Epsilon: 0.751\n",
      "Episode 2/1000, Total Reward: -86.7523770698207, Epsilon: 0.508\n",
      "Episode 3/1000, Total Reward: -215.56334519128535, Epsilon: 0.152\n",
      "Episode 4/1000, Total Reward: -247.0729990098024, Epsilon: 0.071\n",
      "Episode 5/1000, Total Reward: -188.09204252705092, Epsilon: 0.039\n",
      "Episode 6/1000, Total Reward: -239.36442458816424, Epsilon: 0.021\n",
      "Episode 7/1000, Total Reward: -116.87168398339173, Epsilon: 0.010\n",
      "Episode 8/1000, Total Reward: -137.96694913653377, Epsilon: 0.010\n",
      "Episode 9/1000, Total Reward: -97.61610444942764, Epsilon: 0.010\n",
      "Episode 10/1000, Total Reward: -114.57392850325809, Epsilon: 0.010\n",
      "Episode 11/1000, Total Reward: -188.17676545304522, Epsilon: 0.010\n",
      "Episode 12/1000, Total Reward: -125.32645196870855, Epsilon: 0.010\n",
      "Episode 13/1000, Total Reward: -176.62015795456284, Epsilon: 0.010\n",
      "Episode 14/1000, Total Reward: -135.0992580986205, Epsilon: 0.010\n",
      "Episode 15/1000, Total Reward: -191.52391718326783, Epsilon: 0.010\n",
      "Episode 16/1000, Total Reward: -192.28813186469876, Epsilon: 0.010\n",
      "Episode 17/1000, Total Reward: -66.58172118239688, Epsilon: 0.010\n",
      "Episode 18/1000, Total Reward: 183.72381021351765, Epsilon: 0.010\n",
      "Episode 19/1000, Total Reward: -172.05580123402848, Epsilon: 0.010\n",
      "Episode 20/1000, Total Reward: -85.97182571873012, Epsilon: 0.010\n",
      "Episode 21/1000, Total Reward: -220.65142258143197, Epsilon: 0.010\n",
      "Episode 22/1000, Total Reward: -119.60886218816833, Epsilon: 0.010\n",
      "Episode 23/1000, Total Reward: -85.4081363790109, Epsilon: 0.010\n",
      "Episode 24/1000, Total Reward: -113.53530178920028, Epsilon: 0.010\n",
      "Episode 25/1000, Total Reward: -117.48458073503696, Epsilon: 0.010\n",
      "Episode 26/1000, Total Reward: -137.52102122527236, Epsilon: 0.010\n",
      "Episode 27/1000, Total Reward: -160.03632104277762, Epsilon: 0.010\n",
      "Episode 28/1000, Total Reward: -180.61829196037513, Epsilon: 0.010\n",
      "Episode 29/1000, Total Reward: -292.11125265798347, Epsilon: 0.010\n",
      "Episode 30/1000, Total Reward: -210.07078622394494, Epsilon: 0.010\n",
      "Episode 31/1000, Total Reward: -190.49874107172656, Epsilon: 0.010\n",
      "Episode 32/1000, Total Reward: -238.62777850204134, Epsilon: 0.010\n",
      "Episode 33/1000, Total Reward: -144.11176899713223, Epsilon: 0.010\n",
      "Episode 34/1000, Total Reward: -157.77228669682393, Epsilon: 0.010\n",
      "Episode 35/1000, Total Reward: 14.452677905155397, Epsilon: 0.010\n",
      "Episode 36/1000, Total Reward: -27.75611921554774, Epsilon: 0.010\n",
      "Episode 37/1000, Total Reward: -261.42099634208034, Epsilon: 0.010\n",
      "Episode 38/1000, Total Reward: -320.4958564171122, Epsilon: 0.010\n",
      "Episode 39/1000, Total Reward: -339.2145283958638, Epsilon: 0.010\n",
      "Episode 40/1000, Total Reward: -71.78749270163267, Epsilon: 0.010\n",
      "Episode 41/1000, Total Reward: -275.54708666410573, Epsilon: 0.010\n",
      "Episode 42/1000, Total Reward: -110.7966249396885, Epsilon: 0.010\n",
      "Episode 43/1000, Total Reward: -446.3889421370807, Epsilon: 0.010\n",
      "Episode 44/1000, Total Reward: -289.69318316931776, Epsilon: 0.010\n",
      "Episode 45/1000, Total Reward: -312.9006085345926, Epsilon: 0.010\n",
      "Episode 46/1000, Total Reward: -287.24129435502505, Epsilon: 0.010\n",
      "Episode 47/1000, Total Reward: -68.44683287849665, Epsilon: 0.010\n",
      "Episode 48/1000, Total Reward: -286.42405710405126, Epsilon: 0.010\n",
      "Episode 49/1000, Total Reward: -57.38631016402127, Epsilon: 0.010\n",
      "Episode 50/1000, Total Reward: -329.1394494917919, Epsilon: 0.010\n",
      "Episode 51/1000, Total Reward: -347.7161222987258, Epsilon: 0.010\n",
      "Episode 52/1000, Total Reward: -317.07202111616937, Epsilon: 0.010\n",
      "Episode 53/1000, Total Reward: -307.6940526396546, Epsilon: 0.010\n",
      "Episode 54/1000, Total Reward: -321.9495201333873, Epsilon: 0.010\n",
      "Episode 55/1000, Total Reward: -315.97880051838166, Epsilon: 0.010\n",
      "Episode 56/1000, Total Reward: -313.34442869890614, Epsilon: 0.010\n",
      "Episode 57/1000, Total Reward: -441.42720204191096, Epsilon: 0.010\n",
      "Episode 58/1000, Total Reward: -438.12510621394085, Epsilon: 0.010\n",
      "Episode 59/1000, Total Reward: -371.6223253484174, Epsilon: 0.010\n",
      "Episode 60/1000, Total Reward: -407.1648384227339, Epsilon: 0.010\n",
      "Episode 61/1000, Total Reward: -300.0721659918301, Epsilon: 0.010\n",
      "Episode 62/1000, Total Reward: -188.35311032927507, Epsilon: 0.010\n",
      "Episode 63/1000, Total Reward: -244.5219513854546, Epsilon: 0.010\n",
      "Episode 64/1000, Total Reward: -102.13568737729639, Epsilon: 0.010\n",
      "Episode 65/1000, Total Reward: -280.35389215523765, Epsilon: 0.010\n",
      "Episode 66/1000, Total Reward: -357.987273329682, Epsilon: 0.010\n",
      "Episode 67/1000, Total Reward: -300.8130691853898, Epsilon: 0.010\n",
      "Episode 68/1000, Total Reward: -105.26614639551848, Epsilon: 0.010\n",
      "Episode 69/1000, Total Reward: -356.6554021743421, Epsilon: 0.010\n",
      "Episode 70/1000, Total Reward: -298.4210421710412, Epsilon: 0.010\n",
      "Episode 71/1000, Total Reward: -84.51410067220515, Epsilon: 0.010\n",
      "Episode 72/1000, Total Reward: -289.7586877223242, Epsilon: 0.010\n",
      "Episode 73/1000, Total Reward: -224.7205633907992, Epsilon: 0.010\n",
      "Episode 74/1000, Total Reward: -264.92593785042936, Epsilon: 0.010\n",
      "Episode 75/1000, Total Reward: -59.52260239757173, Epsilon: 0.010\n",
      "Episode 76/1000, Total Reward: -185.15017884711563, Epsilon: 0.010\n",
      "Episode 77/1000, Total Reward: -197.37997463205832, Epsilon: 0.010\n",
      "Episode 78/1000, Total Reward: -232.97256582605465, Epsilon: 0.010\n",
      "Episode 79/1000, Total Reward: -67.09878607468448, Epsilon: 0.010\n",
      "Episode 80/1000, Total Reward: -31.434841727640432, Epsilon: 0.010\n",
      "Episode 81/1000, Total Reward: -217.53873171453694, Epsilon: 0.010\n",
      "Episode 82/1000, Total Reward: -206.85147239121113, Epsilon: 0.010\n",
      "Episode 83/1000, Total Reward: -65.48656764316621, Epsilon: 0.010\n",
      "Episode 84/1000, Total Reward: -114.39467774445549, Epsilon: 0.010\n",
      "Episode 85/1000, Total Reward: -194.38208843959504, Epsilon: 0.010\n",
      "Episode 86/1000, Total Reward: -110.85990719572226, Epsilon: 0.010\n",
      "Episode 87/1000, Total Reward: -220.44099091775934, Epsilon: 0.010\n",
      "Episode 88/1000, Total Reward: -78.37552806012607, Epsilon: 0.010\n",
      "Episode 89/1000, Total Reward: -190.99607981432797, Epsilon: 0.010\n",
      "Episode 90/1000, Total Reward: -240.2962958341001, Epsilon: 0.010\n",
      "Episode 91/1000, Total Reward: -92.52882259328328, Epsilon: 0.010\n",
      "Episode 92/1000, Total Reward: -162.5602414649066, Epsilon: 0.010\n",
      "Episode 93/1000, Total Reward: -120.98569512468573, Epsilon: 0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    116\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(policy_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_lunar_lander.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     99\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 100\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update state and epsilon\u001b[39;00m\n\u001b[1;32m    103\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/torch/optim/adam.py:369\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    370\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[1;32m    371\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Define the Q-network (Neural Network)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "target_update_freq = 10\n",
    "max_episodes = 1000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize networks, optimizer, and loss function\n",
    "policy_net = DQN(state_dim, action_dim).float()\n",
    "target_net = DQN(state_dim, action_dim).float()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def select_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "# Train the model\n",
    "def train():\n",
    "    global epsilon\n",
    "    for episode in range(max_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store the experience in replay buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Sample a batch from replay buffer\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.tensor(states, dtype=torch.float32)\n",
    "                actions = torch.tensor(actions, dtype=torch.int64)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32)\n",
    "                \n",
    "                # Q-values from current policy network\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # Q-values from target network\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "                    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update state and epsilon\n",
    "            state = next_state\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{max_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "# Run training\n",
    "train()\n",
    "\n",
    "# Save the model\n",
    "torch.save(policy_net.state_dict(), 'dqn_lunar_lander.pth')\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
