{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9694017,"sourceType":"datasetVersion","datasetId":5926950},{"sourceId":9695710,"sourceType":"datasetVersion","datasetId":5928262}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.functional as F\nimport torch.nn as nn\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras import Model, Input, layers, models, optimizers\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom pprint import pprint\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from os import linesep\nimport string\n\n# Read the file\nfile_path = '/kaggle/input/text-for-next-word-predictor/leo tolstoy - war and peace.txt'\n\n# Open and read the contents of the file\nwith open(file_path, 'r', encoding='utf-8') as file:\n    text = file.read()\n\nfiltered_text = re.sub(r'-', ' ', text)\nfiltered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\nfiltered_text = re.sub(r'\\.{1,}', '', filtered_text)\nfiltered_text = filtered_text.lower()\n\nwords=[]\nfor (word) in filtered_text.split():\n    if word not in words:\n        words.append(word)\n\npara=filtered_text.split(\"\\n\\n\")\nprint(\"Total no. of para: \", len(para))\nprint(\"Total unique words: \", len(words))","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stoi={s:i+1 for i,s in enumerate(words)}\nstoi['.']=0\nitos={i:s for s,i in stoi.items()}\nprint(len(itos))","metadata":{"_kg_hide-output":false,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter\nblock_size=5 # context_length: how many words do we take to predict the next one\n\n# X and Y matrices to store the data for training\n# X stores the half lines\n# Y stores the next word\nX,Y=[],[]\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor p in para:\n  context=[0]*block_size\n\n  for word in p.split():\n    word=word.rstrip(string.punctuation)\n    ix=stoi[word]\n    X.append(context)\n    Y.append(ix)\n    # print(' '.join(itos[i] for i in context), '--->', itos[ix])\n    context = context[1:] + [ix]\n\n\n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n\nX.shape, Y.shape, X.dtype, Y.dtype","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emb_dim = 64 # Hyperparameter\n\n# Embedding layer\nemb = Embedding(input_dim=len(stoi), output_dim=emb_dim)\n\n# Since we're not creating a model, just initialize the embedding layer by calling it on some dummy input\nemb(tf.constant([[0]]))  # Initialize with a dummy input, similar to `.to(device)` in PyTorch\n\n# Print the embedding layer and weights\nprint(emb.get_weights()[0].shape)\n","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\n\nclass Next_Word_Predictor(Model):\n    def __init__(self, block_size, vocab_size, emb_dim, hidden_dim):\n        super(Next_Word_Predictor, self).__init__()\n        self.block_size = block_size\n        self.emb_dim = emb_dim\n        self.emb = layers.Embedding(vocab_size, emb_dim)\n        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n        self.dense2 = layers.Dense(hidden_dim, activation='relu')\n        self.dense3 = layers.Dense(hidden_dim, activation='relu')\n        self.dense4 = layers.Dense(vocab_size, activation='softmax')  # Output layer\n\n    def call(self, x):\n        x = self.emb(x)\n\n        # Reshape to match the input shape for the dense layers\n        x = tf.reshape(x, (tf.shape(x)[0], self.block_size * self.emb_dim))\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n        x = self.dense4(x)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate names from untrained model\n\n\ndef generate_next_words(model, itos, stoi, content, block_size, k=10, max_len=10):\n    context = content.lower()\n    context = re.sub('[^a-zA-Z0-9 \\.]', '', context)\n    context = [stoi[word.strip(string.punctuation)] for word in context.split()]\n\n    if len(context) <= block_size:\n        context = [0] * (block_size - len(context)) + context\n    elif len(context) > block_size:\n        context = context[-block_size:]\n\n    for i in range(k):\n        x = np.array(context).reshape(1, -1)\n        y_pred = model(x)\n        logits = y_pred.numpy()\n        \n        ix = tf.random.categorical(logits, num_samples=1).numpy()[0, 0]\n        word = itos[ix]\n        content += \" \" + word\n        context = context [1:] + [ix]\n        \n    return content\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_np = X.cpu().numpy()  \nY_np = Y.cpu().numpy()  \n\n# Define and compile the Keras model\nmodel = Next_Word_Predictor(block_size, len(stoi), emb_dim, 1024)\nmodel.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n\n# Mini-batch training parameters\nbatch_size = 1024\nprint_every = 100\nmodel.fit(X_np, Y_np, batch_size=batch_size, epochs=20, verbose=1)","metadata":{"trusted":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate names from trained model\n\npara=\" \"\ncontent=input(\"Enter some context: \")\nk=int(input(\"Enter no. of words to be generated: \"))\nfor i in range(10):\n    para+=generate_next_words(model, itos, stoi, content, block_size, k)\n    para+=\"\\n\\n\"\nprint(para)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_weights = model.emb.weights[0].numpy()\nprint(embedding_weights.shape)\n# Reduce dimensionality using t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_tsne = tsne.fit_transform(embedding_weights)\n\n# Visualize embeddings\nplt.figure(figsize=(10, 8))\nplt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5)\nplt.title('t-SNE Visualization of Embeddings')  \nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}